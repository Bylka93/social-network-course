{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing some extra cleaning to the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('also')\n",
    "\n",
    "parking = pd.read_csv('parking_final.csv')\n",
    "parkinglot = pd.read_csv('parkinglot_final.csv')\n",
    "parkinggarage = pd.read_csv('parkinggarage_final.csv')\n",
    "\n",
    "# Lower-case\n",
    "for col in ['text', 'tweet_hashtags', 'location']:\n",
    "    parking[col] = parking[col].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "    parkinglot[col] = parkinglot[col].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "    parkinggarage[col] = parkinggarage[col].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    filtered_text = []\n",
    "    for t in text:\n",
    "        if type(t) == str:\n",
    "            text_tokens = word_tokenize(t)\n",
    "\n",
    "            tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "\n",
    "            filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "            filtered_text.append(filtered_sentence)\n",
    "        else:\n",
    "            filtered_text.append(t)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking['text'] = remove_stop_words(parking.text)\n",
    "parkinglot['text'] = remove_stop_words(parkinglot.text)\n",
    "parkinggarage['text'] = remove_stop_words(parkinggarage.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_hashtags(hashtags):\n",
    "    filtered_hashtags = []\n",
    "    for ht in hashtags:\n",
    "        if type(ht) == str:\n",
    "            text_tokens = ht.split(' ')\n",
    "            tokens_without_hash = [x.lstrip('#') for x in text_tokens]\n",
    "        \n",
    "            tokens_without_sw = [word for word in tokens_without_hash if not word in stop_words]\n",
    "            added_hashtags = [''.join(['#', x]) for x in tokens_without_sw]\n",
    "            \n",
    "            filtered_sentence = (' ').join(added_hashtags)\n",
    "            filtered_hashtags.append(filtered_sentence)\n",
    "        else:\n",
    "            filtered_hashtags.append(ht)\n",
    "    return filtered_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking['tweet_hashtags'] = remove_stopwords_from_hashtags(parking.tweet_hashtags)\n",
    "parkinglot['tweet_hashtags'] = remove_stopwords_from_hashtags(parkinglot.tweet_hashtags)\n",
    "parkinggarage['tweet_hashtags'] = remove_stopwords_from_hashtags(parkinggarage.tweet_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    filtered_text = []\n",
    "    for t in text:\n",
    "        if type(t) == str:\n",
    "            text_parts = list(t)\n",
    "\n",
    "            parts_without_punc = [part for part in text_parts if not part in string.punctuation]\n",
    "\n",
    "            filtered_sentence = ('').join(parts_without_punc)\n",
    "            filtered_text.append(filtered_sentence)\n",
    "        else:\n",
    "            filtered_text.append(t)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking['text'] = remove_punctuation(parking.text)\n",
    "parkinglot['text'] = remove_punctuation(parkinglot.text)\n",
    "parkinggarage['text'] = remove_punctuation(parkinggarage.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some extra removals\n",
    "remove_these = ['\\x99', '\\x98', '\\x97', '\\x96', '\\x95', '\\x94', '\\x93', '\\x92', '\\x91', 'Â·']\n",
    "remove_dict = {ord(rm): '' for rm in remove_these}\n",
    "\n",
    "parking['text'] = parking['text'].apply(lambda x: x.translate(remove_dict) if type(x) == str else x)\n",
    "parkinglot['text'] = parkinglot['text'].apply(lambda x: x.translate(remove_dict) if type(x) == str else x)\n",
    "parkinggarage['text'] = parkinggarage['text'].apply(lambda x: x.translate(remove_dict) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parking.to_csv('parking_final_cleaned.csv', index=False)\n",
    "#parkinglot.to_csv('parkinglot_final_cleaned.csv', index=False)\n",
    "#parkinggarage.to_csv('parkinggarage_final_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the datasets and limiting the amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([parking, parkinglot, parkinggarage])\n",
    "combined['time'] = pd.to_datetime(combined['time'])\n",
    "combined = combined.reset_index()\n",
    "combined.drop(columns=['index'], inplace=True)\n",
    "combined.drop_duplicates(['time', 'user_id', 'text'], inplace=True) # Removing duplicate tweets\n",
    "combined = combined[(combined['time'] >= pd.to_datetime('2020-01-01'))] # Taking only the data from this year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv('all_hashtags_combined.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also limiting the data for each hashtag data file \n",
    "parking['time'] = pd.to_datetime(parking['time'])\n",
    "parkinglot['time'] = pd.to_datetime(parkinglot['time'])\n",
    "parkinggarage['time'] = pd.to_datetime(parkinggarage['time'])\n",
    "\n",
    "parking = parking[(parking['time'] >= pd.to_datetime('2020-01-01'))]\n",
    "parkinglot = parkinglot[(parkinglot['time'] >= pd.to_datetime('2020-01-01'))]\n",
    "parkinggarage = parkinggarage[(parkinggarage['time'] >= pd.to_datetime('2020-01-01'))]\n",
    "\n",
    "# Saving the hashtag files\n",
    "parking.to_csv('parking_from_january.csv', index = False)\n",
    "parkinglot.to_csv('parkinglot_from_january.csv', index = False)\n",
    "parkinggarage.to_csv('parkinggarage_from_january.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
