{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os\n",
    "import GetOldTweets3 as got\n",
    "import os\n",
    "import pandas as pd\n",
    "#!pip install textblob\n",
    "#!pip install tweepy\n",
    "import tweepy\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "#import preprocessor as p\n",
    "import preprocessor.api as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Twitter credentials for the app\n",
    "#consumer_key = ''\n",
    "#consumer_secret = ''\n",
    "#access_key= ''\n",
    "#access_secret = ''\n",
    "\n",
    "#pass twitter credentials to tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mrhod clean_tweets()\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "\n",
    "    #after tweepy preprocessing the colon left remain after removing mentions\n",
    "    #or RT sign in the beginning of the tweet\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    #Added after seing the data most dominant characters \n",
    "    tweet = re.sub(r'™â¢¦€â€™',' ', tweet)\n",
    "\n",
    "\n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "\n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parkinglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "#!pip install textblob\n",
    "#!pip install tweepy\n",
    "import tweepy\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "#import preprocessor as p\n",
    "import preprocessor.api as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tag = 'parkinglot' # tag as command line argument\n",
    "hashtag = ''.join(['#', tag]) # Adding the hash\n",
    "\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(hashtag).setSince(\"2020-01-01\").setUntil(\"2020-04-11\").setLang('en') # Search for hashtag, set starting day, set language\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria) # Getting the tweet objects\n",
    "\n",
    "# Column names\n",
    "colnames = ['time', 'id','username' , 'text', 'retweets', \n",
    "'favorites', 'tweet_hashtags']\n",
    "        \n",
    "# Open/Create a file to append data\n",
    "with open(f'{tag}.csv', 'a', newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(colnames)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            clean_text = p.clean(tweet.text)\n",
    "\n",
    "            #call clean_tweet method for extra preprocessing\n",
    "            filtered_tweet=clean_tweets(clean_text)\n",
    "            csvWriter.writerow([tweet.date, tweet.id,tweet.username, filtered_tweet, tweet.retweets, tweet.favorites, tweet.hashtags])\n",
    "        except UnicodeEncodeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parkinggarage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "#!pip install textblob\n",
    "#!pip install tweepy\n",
    "import tweepy\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "#import preprocessor as p\n",
    "import preprocessor.api as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tag = 'parkinggarage' # tag as command line argument\n",
    "hashtag = ''.join(['#', tag]) # Adding the hash\n",
    "\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(hashtag).setSince(\"2020-01-01\").setUntil(\"2020-04-11\").setLang('en') # Search for hashtag, set starting day, set language\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria) # Getting the tweet objects\n",
    "\n",
    "# Column names\n",
    "colnames = ['time', 'id','username' , 'text', 'retweets', \n",
    "'favorites', 'tweet_hashtags']\n",
    "        \n",
    "# Open/Create a file to append data\n",
    "with open(f'{tag}.csv', 'a', newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(colnames)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            clean_text = p.clean(tweet.text)\n",
    "\n",
    "            #call clean_tweet method for extra preprocessing\n",
    "            filtered_tweet=clean_tweets(clean_text)\n",
    "            csvWriter.writerow([tweet.date, tweet.id,tweet.username, filtered_tweet, tweet.retweets, tweet.favorites, tweet.hashtags])\n",
    "        except UnicodeEncodeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "#!pip install textblob\n",
    "#!pip install tweepy\n",
    "import tweepy\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "#import preprocessor as p\n",
    "import preprocessor.api as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tag = 'parking' # tag as command line argument\n",
    "hashtag = ''.join(['#', tag]) # Adding the hash\n",
    "\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(hashtag).setSince(\"2020-01-01\").setUntil(\"2020-01-11\").setLang('en') # Search for hashtag, set starting day, set language\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria) # Getting the tweet objects\n",
    "\n",
    "# Column names\n",
    "colnames = ['time', 'id','username' , 'text', 'retweets', \n",
    "'favorites', 'tweet_hashtags']\n",
    "        \n",
    "# Open/Create a file to append data\n",
    "with open(f'{tag}.csv', 'a', newline='') as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(colnames)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            clean_text = p.clean(tweet.text)\n",
    "\n",
    "            #call clean_tweet method for extra preprocessing\n",
    "            filtered_tweet=clean_tweets(clean_text)\n",
    "            csvWriter.writerow([tweet.date, tweet.id,tweet.username, filtered_tweet, tweet.retweets, tweet.favorites, tweet.hashtags])\n",
    "        except UnicodeEncodeError:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
